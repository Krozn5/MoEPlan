{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, psycopg2, json, pyparsing as pp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'tpcds1gb'\n",
    "user = 'postgres'\n",
    "host = '162.105.86.21'\n",
    "password = '123456'\n",
    "port = '5435'\n",
    "simple_queries = list(range(1, 100))\n",
    "# simple_queries = [3, 7, 12, 20, 26, 37, 42, 43, 50, 55, 62, 84, 91, 96, 99, 18, 27, 52, 82, 98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query():\n",
    "\tqueries = []\n",
    "\twith psycopg2.connect(host=host, port=port, database=database, user=user, password=password) as conn:\n",
    "\t\twith conn.cursor() as cur:\n",
    "\t\t\tcur.execute('SELECT * FROM public.queries')\n",
    "\t\t\tq_list = list(cur.fetchall())\n",
    "\t\t\tq_list = sorted(q_list, key=lambda x:x[0])\n",
    "\t\t\tfor q in tqdm(q_list):\n",
    "\t\t\t\t# print(q[0])\n",
    "\t\t\t\tif q[0] in simple_queries:\n",
    "\t\t\t\t\tquery = q[3].split('\\n')\n",
    "\t\t\t\t\tquery = filter(lambda x:not x.startswith('--'), query)\n",
    "\t\t\t\t\tquery = ' '.join(query)\n",
    "\t\t\t\t\tqueries.append(query)\n",
    "\treturn queries\n",
    "\n",
    "def get_train_plan():\n",
    "\tids, jsons = [], []\n",
    "\twith psycopg2.connect(host=host, port=port, database=database, user=user, password=password) as conn:\n",
    "\t\twith conn.cursor() as cur:\n",
    "\t\t\tcur.execute('SELECT * FROM public.queries')\n",
    "\t\t\tq_list = list(cur.fetchall())\n",
    "\t\t\tq_list = sorted(q_list, key=lambda x:x[0])\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor q in tqdm(q_list):\n",
    "\t\t\t\t# print(q[0])\n",
    "\t\t\t\tif q[0] in simple_queries:\n",
    "\t\t\t\t\tids.append(counter)\n",
    "\t\t\t\t\tcounter += 1\n",
    "\t\t\t\t\tcur.execute('explain (format json, analyze, buffers, verbose) ' + q[3][:q[3].find(';')+1])\n",
    "\t\t\t\t\tjsons.append(json.dumps(cur.fetchall()[0][0][0]))\n",
    "\treturn pd.DataFrame({'id': ids, 'json': jsons})\n",
    "\n",
    "def extract_plan(plan):\n",
    "    table, join, predicate = [], [], []\n",
    "    keys = plan.keys()\n",
    "    if 'Relation Name' in plan:\n",
    "        table.append(plan['Relation Name']+' '+plan['Alias'])\n",
    "    for key in keys:\n",
    "        if 'Cond' in key and 'Join' in plan['Node Type']:\n",
    "            join.append(plan[key][1:-1].replace(' ', ''))\n",
    "        if 'Filter' in key and 'by' not in key:\n",
    "            predicate.append(plan[key])\n",
    "    if 'Plans' in keys:\n",
    "        for child in plan['Plans']:\n",
    "            t, j, p = extract_plan(child)\n",
    "            table += t\n",
    "            join += j\n",
    "            predicate += p\n",
    "    return table, join, predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_parens(s):\n",
    "#     print('find_parens', s)\n",
    "#     toret = {}\n",
    "#     pstack = []\n",
    "#     for i, c in enumerate(s):\n",
    "#         if c == '(':\n",
    "#             pstack.append(i)\n",
    "#         elif c == ')':\n",
    "#             if len(pstack) == 0:\n",
    "#                 raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "#             toret[pstack.pop()] = i\n",
    "#     if len(pstack) > 0:\n",
    "#         raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "#     return toret\n",
    "\n",
    "# def find_parens_with_exp(s, limit=4):\n",
    "#     for j in range(limit):\n",
    "#         ss = s[j:]\n",
    "#         try:\n",
    "#             parens = find_parens(ss)\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             pass\n",
    "#     else:\n",
    "#         for j in range(limit):\n",
    "#             ss = s[:-j]\n",
    "#             try:\n",
    "#                 parens = find_parens(ss)\n",
    "#                 break\n",
    "#             except Exception as e:\n",
    "#                 pass\n",
    "#     return ss, parens\n",
    "\n",
    "# def extract_predicates(predicates):\n",
    "#     ps = []\n",
    "#     for i, plist in tqdm(enumerate(predicates)):\n",
    "#         predicate, ppredicate = [], []\n",
    "#         for p in plist:\n",
    "#             p_parens = find_parens(p)\n",
    "#             while 0 in p_parens and p_parens[0] == len(p) - 1:\n",
    "#                 p = p[1:-1]\n",
    "#                 p_parens = find_parens(p)\n",
    "#             try:\n",
    "#                 for pp in p.split('AND'):\n",
    "#                     # pp_parens = find_parens(pp)\n",
    "#                     pp, pp_parens = find_parens_with_exp(pp.strip())\n",
    "#                     while 0 in pp_parens and pp_parens[0] == len(pp) - 1:\n",
    "#                         pp = pp[1:-1]\n",
    "#                         pp_parens = find_parens(pp)\n",
    "#                     ppredicate += [s.strip() for s in pp.split('OR')]\n",
    "#             except Exception as e:\n",
    "#                 for pp in p.split('OR'):\n",
    "#                     # pp_parens = find_parens(pp)\n",
    "#                     pp, pp_parens = find_parens_with_exp(pp.strip())\n",
    "#                     while 0 in pp_parens and pp_parens[0] == len(pp) - 1:\n",
    "#                         pp = pp[1:-1]\n",
    "#                         pp_parens = find_parens(pp)\n",
    "#                     ppredicate += [s.strip() for s in pp.split('AND')]\n",
    "#         for p in ppredicate:\n",
    "#             p_parens = find_parens(p)\n",
    "#             while 0 in p_parens and p_parens[0] == len(p) - 1:\n",
    "#                 p = p[1:-1]\n",
    "#                 p_parens = find_parens(p)\n",
    "#             for op in ['>=', '<=', '~~', '>', '<', '=']:\n",
    "#                 if p.find(' '+op+' ') != -1:\n",
    "#                     break\n",
    "#             else:\n",
    "#                 print(f'Not supported query {i+1}')\n",
    "#                 continue\n",
    "#                 # raise Exception(f'Unrecognized op in {p}')\n",
    "#             p = p.split(' '+op+' ')\n",
    "#             predicate += [p[0], op, p[1]]\n",
    "#         ps.append(',,'.join(predicate))\n",
    "#     return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = pp.one_of(\">= <= != <> ~~ > < =\").set_name(\"operator\")\n",
    "number = pp.pyparsing_common.number()\n",
    "# there is no space character in this pp.Word expression\n",
    "word = pp.Word(pp.alphanums + \"_-*1234567890,.'$\", pp.alphanums + \"_-*1234567890,.:'[]\\{\\}%/$ \")\n",
    "# word = pp.Regex(r\"^[a-zA-Z0-9\\_\\-\\*\\,\\.\\'\\$][a-zA-Z0-9\\_\\-\\*\\,\\.\\:\\'\\[\\]\\{\\}\\%\\/\\$\\ ]+[a-zA-Z0-9\\_\\-\\*\\,\\.\\:\\'\\[\\]\\{\\}\\%\\/\\$]$\")\n",
    "word_type = pp.Word(pp.alphas, pp.alphas + ' ')\n",
    "\n",
    "# term = 'sum(' + word + ')' | 'avg(' + word + ')' | '(' + word + ')::' + type | word | number | pp.quotedString\n",
    "term = '(' + word + ')::' + word_type | word + '::' + word_type | word | number | pp.quotedString\n",
    "# term = term.setParseAction(lambda t:t[0].strip())\n",
    "condition = pp.Group(term + operator + term)\n",
    "\n",
    "\n",
    "expr = pp.infix_notation(condition,\n",
    "         [('NOT', 1, pp.opAssoc.RIGHT,),\n",
    "          ('AND', 2, pp.opAssoc.LEFT,),\n",
    "          ('OR', 2, pp.opAssoc.LEFT,)])\n",
    "\n",
    "def extract_predicates(ps):\n",
    "    predicate = []\n",
    "    if 'AND' in ps or 'OR' in ps:\n",
    "        for p in ps:\n",
    "            if p not in ['AND', 'OR']:\n",
    "                predicate += extract_predicates(p)\n",
    "    else:\n",
    "        for i, s in enumerate(ps):\n",
    "            if s in ['>=', '<=', '!=', '<>', '~~', '>', '<', '=']:\n",
    "                op_pos = i\n",
    "                break\n",
    "        else:\n",
    "            raise Exception(f'No op found in {ps}')\n",
    "        predicate += [(''.join(ps[:op_pos])).rstrip(), ps[op_pos], ''.join(ps[op_pos+1:])]\n",
    "    return predicate\n",
    "\n",
    "def predicates2str(preds):\n",
    "    ppreds = []\n",
    "    for p in preds:\n",
    "        try:\n",
    "            ppreds.append(expr.parse_string(p, parseAll=True)[0].as_list())\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    ppreds = [',,'.join(extract_predicates(p)) for p in ppreds]\n",
    "    return ',,'.join(ppreds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthetic.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 98959.98it/s]\n"
     ]
    }
   ],
   "source": [
    "query_path = 'workloads'\n",
    "if not os.path.exists(query_path):\n",
    "    os.makedirs(query_path)\n",
    "queries = get_query()\n",
    "with open(f'{query_path}/synthetic.sql', 'w') as f:\n",
    "    f.write('\\n'.join(queries))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_plan.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [02:28<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "plan_path = 'plan_and_cost'\n",
    "if not os.path.exists(plan_path):\n",
    "    os.makedirs(plan_path)\n",
    "df = get_train_plan()\n",
    "df.to_csv(f'{plan_path}/train_plan.csv', index=False, sep=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthetic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 4313.51it/s]\n"
     ]
    }
   ],
   "source": [
    "query_path = 'workloads'\n",
    "plan_path = 'plan_and_cost'\n",
    "df = pd.read_csv(f'{plan_path}/train_plan.csv', sep=',')\n",
    "tables, joins, predicates, cards = [], [], [], []\n",
    "for index, row in tqdm(list(df.iterrows())):\n",
    "\tplan = json.loads(row['json'])\n",
    "\ttable, join, predicate = extract_plan(plan=plan['Plan'])\n",
    "\ttables.append(','.join(table))\n",
    "\tjoins.append(','.join(join))\n",
    "\tpredicates.append(predicate)\n",
    "\tcards.append(plan['Plan']['Actual Rows'])\n",
    "# with open('predicates.json', 'w') as f:\n",
    "# \tjson.dump(predicates, f, indent=4)\n",
    "predicates = list(map(predicates2str, predicates))\n",
    "df = pd.DataFrame({'table':tables, 'join':joins, 'predicate':predicates, 'card':cards})\n",
    "df.to_csv(f'{query_path}/synthetic.csv', index=False, sep='#', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = pp.one_of(\">= <= != <> ~~ > < =\").set_name(\"operator\")\n",
    "number = pp.pyparsing_common.number()\n",
    "# there is no space character in this pp.Word expression\n",
    "word = pp.Word(pp.alphanums + \"_-*1234567890,.'$\", pp.alphanums + \"_-*1234567890,.:'[]\\{\\}%/$ \")\n",
    "# word = pp.Regex(r\"^[a-zA-Z0-9\\_\\-\\*\\,\\.\\'\\$][a-zA-Z0-9\\_\\-\\*\\,\\.\\:\\'\\[\\]\\{\\}\\%\\/\\$\\ ]+[a-zA-Z0-9\\_\\-\\*\\,\\.\\:\\'\\[\\]\\{\\}\\%\\/\\$]$\")\n",
    "word_type = pp.Word(pp.alphas, pp.alphas + ' ')\n",
    "\n",
    "# term = 'sum(' + word + ')' | 'avg(' + word + ')' | '(' + word + ')::' + type | word | number | pp.quotedString\n",
    "term = '(' + word + ')::' + word_type | word + '::' + word_type | word | number | pp.quotedString\n",
    "# term = term.setParseAction(lambda t:t[0].strip())\n",
    "condition = pp.Group(term + operator + term)\n",
    "\n",
    "\n",
    "expr = pp.infix_notation(condition,\n",
    "         [('NOT', 1, pp.opAssoc.RIGHT,),\n",
    "          ('AND', 2, pp.opAssoc.LEFT,),\n",
    "          ('OR', 2, pp.opAssoc.LEFT,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:12<00:00,  7.77it/s]\n"
     ]
    }
   ],
   "source": [
    "parsed_predicates = []\n",
    "counter, err_counter = 0, 0\n",
    "for p in tqdm(predicates):\n",
    "    parsed_predicates.append([])\n",
    "    for ppp in p:\n",
    "        counter += 1\n",
    "        try:\n",
    "            pred = expr.parse_string(ppp, parseAll=True)[0].as_list()\n",
    "            parsed_predicates[-1].append(',,'.join(extract_predicates(pred)))\n",
    "        except Exception as e:\n",
    "            err_counter += 1\n",
    "            parsed_predicates[-1].append(f'Error parsing: {ppp}')\n",
    "with open('parsed_predicates.json', 'w') as f:\n",
    "\tjson.dump(parsed_predicates, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(counter)\n",
    "print(err_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "query-former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
